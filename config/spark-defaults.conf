# Spark Configuration
# spark-defaults.conf

# Application Properties
spark.app.name=Financial-Lakehouse-Pipeline

# Master
spark.master=spark://spark-master:7077

# Memory Configuration
spark.executor.memory=4g
spark.driver.memory=2g
spark.executor.cores=2

# Dynamic Allocation
spark.dynamicAllocation.enabled=true
spark.dynamicAllocation.minExecutors=1
spark.dynamicAllocation.maxExecutors=5
spark.dynamicAllocation.initialExecutors=2

# Shuffle Configuration
spark.shuffle.service.enabled=true
spark.shuffle.compress=true
spark.shuffle.spill.compress=true

# SQL Configuration
spark.sql.adaptive.enabled=true
spark.sql.adaptive.coalescePartitions.enabled=true
spark.sql.adaptive.skewJoin.enabled=true
spark.sql.adaptive.localShuffleReader.enabled=true

# Serialization
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.kryoserializer.buffer.max=512m

# Compression
spark.io.compression.codec=snappy
spark.rdd.compress=true

# Broadcast
spark.broadcast.compress=true
spark.broadcast.blockSize=4m

# Network
spark.network.timeout=600s
spark.executor.heartbeatInterval=60s

# Storage
spark.storage.level=MEMORY_AND_DISK_SER
spark.memory.fraction=0.6
spark.memory.storageFraction=0.5

# Logging
spark.eventLog.enabled=false
spark.history.fs.logDirectory=/tmp/spark-events

# Python Configuration
spark.python.worker.memory=1g
spark.pyspark.python=python3
spark.pyspark.driver.python=python3

# Parquet Configuration
spark.sql.parquet.compression.codec=snappy
spark.sql.parquet.mergeSchema=false
spark.sql.parquet.filterPushdown=true

# ORC Configuration
spark.sql.orc.filterPushdown=true
spark.sql.orc.compression.codec=zlib

# Hive Integration
spark.sql.hive.metastore.version=2.3.2
spark.sql.hive.metastore.jars=builtin
spark.sql.catalogImplementation=hive

# Hadoop Configuration
spark.hadoop.fs.defaultFS=hdfs://namenode:9000
spark.hadoop.dfs.client.use.datanode.hostname=false

# UI Configuration
spark.ui.port=4040
spark.ui.enabled=true
spark.ui.killEnabled=true

# Speculation
spark.speculation=false
spark.speculation.interval=100ms
spark.speculation.multiplier=1.5

# Task Configuration
spark.task.maxFailures=4
spark.task.reaper.enabled=true
spark.task.reaper.pollingInterval=10s

# SQL Warehouse
spark.sql.warehouse.dir=/user/hive/warehouse

# Partition Discovery
spark.sql.sources.partitionOverwriteMode=dynamic
spark.sql.sources.partitionColumnTypeInference.enabled=true

# Performance Tuning
spark.default.parallelism=8
spark.sql.shuffle.partitions=200
spark.sql.files.maxPartitionBytes=134217728
spark.sql.files.openCostInBytes=4194304
