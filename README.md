# Financial Data Lakehouse Pipeline

## ğŸ“‹ MÃ´ táº£ dá»± Ã¡n

Há»‡ thá»‘ng tá»± Ä‘á»™ng thu tháº­p, lÆ°u trá»¯ vÃ  phÃ¢n tÃ­ch dá»¯ liá»‡u tá»« cÃ¡c nguá»“n API tÃ i chÃ­nh Ä‘á»ƒ phÃ¢n tÃ­ch má»‘i tÆ°Æ¡ng quan giá»¯a tin tá»©c (Sentiment) vÃ  biáº¿n Ä‘á»™ng giÃ¡ cá»• phiáº¿u.

## ğŸ—ï¸ Kiáº¿n trÃºc há»‡ thá»‘ng

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Sources  â”‚
â”‚  - Polygon API  â”‚
â”‚  - Alpha Vantageâ”‚
â”‚  - SEC API      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Python ETL     â”‚
â”‚  (Extraction)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Hadoop HDFS    â”‚
â”‚  (Data Lake)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Apache Hive    â”‚
â”‚  (Structure)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Apache Spark   â”‚
â”‚  (Processing)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PostgreSQL     â”‚
â”‚  (Analytics DB) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Apache Airflow Ä‘iá»u phá»‘i toÃ n bá»™ pipeline
```

## ğŸš€ CÃ´ng nghá»‡ sá»­ dá»¥ng

- **Python 3.9+**: NgÃ´n ngá»¯ láº­p trÃ¬nh chÃ­nh
- **Apache Airflow 2.7+**: Orchestration vÃ  scheduling
- **Hadoop HDFS 3.3+**: Distributed storage
- **Apache Hive 3.1+**: Data warehousing
- **Apache Spark 3.4+**: Distributed processing
- **PostgreSQL 15+**: Analytical database
- **Docker & Docker Compose**: Containerization

## ğŸ“ Cáº¥u trÃºc dá»± Ã¡n

```
Financial-Data-Lakehouse-Pipeline/
â”œâ”€â”€ docker/                     # Docker configurations
â”‚   â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ hadoop/
â”‚   â”œâ”€â”€ hive/
â”‚   â””â”€â”€ spark/
â”œâ”€â”€ dags/                       # Airflow DAGs
â”‚   â”œâ”€â”€ financial_data_pipeline.py
â”‚   â””â”€â”€ data_quality_check.py
â”œâ”€â”€ scripts/                    # Python scripts
â”‚   â”œâ”€â”€ extractors/            # API data extractors
â”‚   â”‚   â”œâ”€â”€ polygon_extractor.py
â”‚   â”‚   â”œâ”€â”€ alpha_vantage_extractor.py
â”‚   â”‚   â””â”€â”€ sec_extractor.py
â”‚   â”œâ”€â”€ spark_jobs/            # Spark processing jobs
â”‚   â”‚   â”œâ”€â”€ data_cleaning.py
â”‚   â”‚   â”œâ”€â”€ data_transformation.py
â”‚   â”‚   â””â”€â”€ sentiment_analysis.py
â”‚   â””â”€â”€ utils/                 # Utility functions
â”‚       â”œâ”€â”€ hdfs_utils.py
â”‚       â”œâ”€â”€ hive_utils.py
â”‚       â””â”€â”€ config.py
â”œâ”€â”€ sql/                        # SQL scripts
â”‚   â”œâ”€â”€ hive_schemas.sql
â”‚   â””â”€â”€ postgresql_schemas.sql
â”œâ”€â”€ config/                     # Configuration files
â”‚   â”œâ”€â”€ airflow.cfg
â”‚   â”œâ”€â”€ hive-site.xml
â”‚   â””â”€â”€ spark-defaults.conf
â”œâ”€â”€ tests/                      # Unit tests
â”œâ”€â”€ docs/                       # Documentation
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

## ğŸ”§ CÃ i Ä‘áº·t vÃ  Cháº¡y

### YÃªu cáº§u há»‡ thá»‘ng

- Docker Desktop 4.0+
- RAM: Tá»‘i thiá»ƒu 16GB (khuyáº¿n nghá»‹ 32GB)
- Disk: Tá»‘i thiá»ƒu 50GB trá»‘ng
- CPU: 4 cores+

### BÆ°á»›c 1: Clone vÃ  cáº¥u hÃ¬nh

```bash
cd "d:\Financial Data Lakehouse Pipeline"

# Táº¡o file .env cho API keys
cp .env.example .env
# Chá»‰nh sá»­a .env vá»›i API keys cá»§a báº¡n
```

### BÆ°á»›c 2: Khá»Ÿi Ä‘á»™ng há»‡ thá»‘ng

```bash
# Build vÃ  start táº¥t cáº£ services
docker-compose up -d

# Kiá»ƒm tra tráº¡ng thÃ¡i
docker-compose ps
```

### BÆ°á»›c 3: Truy cáº­p cÃ¡c services

- **Airflow UI**: http://localhost:8080 (user: admin, password: admin)
- **Spark Master UI**: http://localhost:8081
- **Hadoop NameNode UI**: http://localhost:9870
- **Hive Server**: localhost:10000

## ğŸ“Š Data Flow

### 1. Extraction (Python)
```python
# Thu tháº­p dá»¯ liá»‡u tá»« APIs
- Polygon API: Stock prices, trades, quotes
- Alpha Vantage: Technical indicators, fundamental data
- SEC API: Company filings, news
```

### 2. Load to HDFS
```bash
# Dá»¯ liá»‡u JSON Ä‘Æ°á»£c lÆ°u vÃ o HDFS
/data/raw/
  â”œâ”€â”€ polygon/YYYY-MM-DD/
  â”œâ”€â”€ alpha_vantage/YYYY-MM-DD/
  â””â”€â”€ sec/YYYY-MM-DD/
```

### 3. Structure with Hive
```sql
-- Táº¡o external tables trÃªn HDFS
-- Partition theo ngÃ y/thÃ¡ng
-- Optimize vá»›i ORC format
```

### 4. Transform with Spark
```python
# Data cleaning, deduplication
# Feature engineering
# Sentiment scoring
# Correlation analysis
```

### 5. Load to PostgreSQL
```sql
-- LÆ°u káº¿t quáº£ phÃ¢n tÃ­ch
-- Táº¡o aggregated tables
-- Create views cho reporting
```

## ğŸ“š API Data Sources

### Polygon.io
- **Endpoint**: Stock aggregates, trades, quotes
- **Frequency**: Real-time to daily
- **Data**: OHLCV, volume, trades

### Alpha Vantage
- **Endpoint**: TIME_SERIES_DAILY, NEWS_SENTIMENT
- **Frequency**: Daily
- **Data**: Technical indicators, news sentiment

### SEC EDGAR
- **Endpoint**: Company filings
- **Frequency**: As filed
- **Data**: 10-K, 10-Q, 8-K filings

## ğŸ”„ Airflow Pipeline Schedule

```python
# Main pipeline: Cháº¡y hÃ ng ngÃ y lÃºc 6:00 AM
DAG: financial_data_pipeline
Schedule: 0 6 * * *

Tasks:
1. extract_polygon_data
2. extract_alpha_vantage_data
3. extract_sec_data
4. load_to_hdfs
5. create_hive_tables
6. spark_data_cleaning
7. spark_transformation
8. spark_sentiment_analysis
9. load_to_postgresql
10. data_quality_check
```

## ğŸ§ª Testing

```bash
# Cháº¡y unit tests
pytest tests/

# Test Airflow DAG
python dags/financial_data_pipeline.py

# Test Spark job locally
spark-submit scripts/spark_jobs/data_cleaning.py
```

## ğŸ“ˆ Monitoring vÃ  Logging

- **Airflow Logs**: Xem trong UI hoáº·c `/logs` directory
- **Spark Logs**: Xem trong Spark UI
- **HDFS Health**: Hadoop NameNode UI
- **Database Metrics**: PostgreSQL queries

## ğŸ“ Kiáº¿n thá»©c há»c Ä‘Æ°á»£c

### 1. Big Data Architecture
- Thiáº¿t káº¿ Data Lake vá»›i HDFS
- ELT vs ETL patterns
- Data partitioning strategies

### 2. Distributed Computing
- Spark RDD, DataFrame operations
- Parallel processing optimization
- Resource management

### 3. Data Warehousing
- Hive table design
- Partitioning vÃ  bucketing
- Query optimization

### 4. Workflow Orchestration
- Airflow DAG design
- Task dependencies
- Error handling vÃ  retry logic

### 5. Containerization
- Multi-container applications
- Service networking
- Volume management

## ğŸ› Troubleshooting

### HDFS khÃ´ng start Ä‘Æ°á»£c
```bash
# Format namenode (CHá»ˆ Láº¦N Äáº¦U)
docker-compose exec namenode hdfs namenode -format
```

### Airflow scheduler khÃ´ng cháº¡y
```bash
# Restart scheduler
docker-compose restart airflow-scheduler
```

### Spark job bá»‹ lá»—i memory
```bash
# TÄƒng memory trong spark-defaults.conf
spark.executor.memory=4g
spark.driver.memory=2g
```

## ğŸ“– TÃ i liá»‡u tham kháº£o

- [Apache Airflow Docs](https://airflow.apache.org/docs/)
- [Apache Spark Docs](https://spark.apache.org/docs/latest/)
- [Hadoop HDFS Docs](https://hadoop.apache.org/docs/stable/)
- [Apache Hive Docs](https://hive.apache.org/)

## ğŸ¤ Contributing

Dá»± Ã¡n nÃ y lÃ  Ä‘á»ƒ há»c táº­p. Báº¡n cÃ³ thá»ƒ:
1. Fork project
2. ThÃªm features má»›i
3. Improve performance
4. Fix bugs

## ğŸ“ License

MIT License - Free to use for learning purposes

## ğŸ‘¤ Author

Dá»± Ã¡n thá»±c hÃ nh Big Data & Data Engineering

---

**Happy Learning! ğŸš€**
